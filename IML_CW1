import os
import numpy as np
from numpy.random import default_rng
import matplotlib.pyplot as plt

def read_dataset(filepath):
    data = np.loadtxt(filepath)
    x = data[:, :-1]
    y_labels = data[:, -1]
    [classes, y] = np.unique(y_labels, return_inverse=True)
    return (x, y, classes)

def split_dataset(x, y, test_proportion, random_generator=default_rng()):

    shuffled_indices = random_generator.permutation(len(x))
    n_test = round(len(x) * test_proportion)
    n_train = len(x) - n_test
    x_train = x[shuffled_indices[:n_train]]
    y_train = y[shuffled_indices[:n_train]]
    x_test = x[shuffled_indices[n_train:]]
    y_test = y[shuffled_indices[n_train:]]
    return (x_train, x_test, y_train, y_test)



(clean_x, clean_y, clean_classes) = read_dataset("clean_dataset.txt")
(noisy_x, noisy_y, noisy_classes) = read_dataset("noisy_dataset.txt")

seed = 60012
rg = default_rng(seed)
x_train_clean, x_test_clean, y_train_clean, y_test_clean = split_dataset(
                                                 clean_x, clean_y,
                                                 test_proportion=0.2,
                                                 random_generator=rg)

x_train_noisy, x_test_noisy, y_train_noisy, y_test_noisy = split_dataset(
                                                 noisy_x, noisy_y,
                                                 test_proportion=0.2,
                                                 random_generator=rg)

class Node:
  def __init__(self, attribute=None, value=None, l_tree=None, r_tree=None, leaf_class=None):
    self.attribute = attribute
    self.value = value
    self.l_tree = l_tree
    self.r_tree = r_tree
    self.leaf_class = leaf_class

def h(label_counts):
  if np.sum(label_counts) == 0:
    return 0
  pk = label_counts / np.sum(label_counts)
  pk = pk[pk > 0]
  return - np.sum(pk * np.log2(pk))

def calc_info_gain(l_sizes, r_sizes):
  h_all = h(r_sizes + l_sizes)
  h_left = h(l_sizes)
  h_right = h(r_sizes)
  left_size = np.sum(l_sizes)
  right_size = np.sum(r_sizes)
  remainder = ((left_size * h_left)/(left_size + right_size)) + ((right_size * h_right)/(left_size + right_size))
  return h_all - remainder

def find_col_split(dataset_x, dataset_y, attribute):
  sorted_indices = np.argsort(dataset_x[:,attribute])
  best_split_val = None
  best_info_gain = 0
  unique_labels, r_sizes = np.unique(dataset_y, return_counts = True)
  l_sizes = np.zeros_like(r_sizes)
  label_to_i = {label: idx for idx, label in enumerate(unique_labels)}
  for i in range(dataset_x.shape[0] - 1):
      if dataset_x[sorted_indices[i], attribute] == dataset_x[sorted_indices[i+1], attribute]:
        continue
      label = label_to_i[dataset_y[sorted_indices[i]]]
      l_sizes[label] += 1
      r_sizes[label] -= 1
      info_gain = calc_info_gain(l_sizes, r_sizes)
      if info_gain > best_info_gain:
          best_info_gain = info_gain
          best_split_val = (dataset_x[sorted_indices[i], attribute] + dataset_x[sorted_indices[i+1], attribute]) / 2
  return best_split_val, best_info_gain


def find_split(dataset_x, dataset_y):
  splits = np.zeros((dataset_x.shape[1], 2))
  for attribute in range(dataset_x.shape[1]):
    splits[attribute] = find_col_split(dataset_x, dataset_y, attribute)
  max_index = np.argmax(splits[:, 1])
  return splits[max_index, 0], max_index

def decision_tree_learning(dataset_x, dataset_y, depth):
  classes = np.unique(dataset_y)
  if classes.shape[0] <= 1:
    return Node(leaf_class=classes[0]), depth
  else:
    split_val, split_attr = find_split(dataset_x, dataset_y)
    split_attr = int(split_attr)
    if split_val == None:
      most_common = np.bincount(dataset_y).argmax()
      return Node(leaf_class=most_common), depth
    pred = dataset_x[:, split_attr] < split_val
    l_branch, l_depth = decision_tree_learning(dataset_x[pred], dataset_y[pred], depth + 1)
    r_branch, r_depth = decision_tree_learning(dataset_x[~pred], dataset_y[~pred], depth + 1)
    node = Node(attribute=split_attr, value=split_val, l_tree=l_branch, r_tree=r_branch)
    return node, max(l_depth, r_depth)


class DecisionTree:
  def fit(self, dataset_x, dataset_y):
    self.tree, _ = decision_tree_learning(dataset_x, dataset_y, 0)
  def predict(self, xs):
    y = np.zeros(xs.shape[0], dtype=int)
    for i, x in enumerate(xs):
      cur = self.tree
      while cur.leaf_class is None:
        if x[cur.attribute] <= cur.value:
          cur = cur.l_tree
        else:
          cur = cur.r_tree
      y[i] = cur.leaf_class
    return y

def plot_tree(node, depth=0, pos=(0, 0), ax=None, max_depth=None):
    x_offset=3
    y_step=3
    if ax is None:
        fig, ax = plt.subplots(figsize=(40, 20))
        ax.set_axis_off()

    x, y = pos
    if max_depth is not None and depth >= max_depth:
        ax.text(
            x, y, "... (truncated)", ha="center", va="center",
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray")
        )
        return

    if node.leaf_class is not None:
        ax.text(x, y, f"Leaf: {node.leaf_class}", ha='center', va='center',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
    else:
        ax.text(x, y, f"x{node.attribute} < {node.value}", ha='center', va='center',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))

        dx = x_offset / (1.75 ** depth)
        dy = y_step

        plot_tree(node.l_tree, depth + 1, pos=(x - dx, y - dy), ax=ax, max_depth=max_depth)
        ax.plot([x, x - dx], [y - 0.2, y - dy + 0.2], 'k-')

        plot_tree(node.r_tree, depth + 1, pos=(x + dx, y - dy), ax=ax, max_depth=max_depth)
        ax.plot([x, x + dx], [y - 0.2, y - dy + 0.2], 'k-')

    if depth == 0:
        plt.show()


#tree trained on entire clean dataset
clean_tree = DecisionTree()
clean_tree.fit(clean_x, clean_y)
plot_tree(clean_tree.tree)

def accuracy(y_test, predictions):
  assert len(y_test) == len(predictions)

  try:
    accuracy = np.sum(y_test == predictions) / len(y_test)
  except ZeroDivisionError:
    accuracy = 0.

  return accuracy

def confusion_matrix(y_gold, y_prediction, class_labels=None):
        # if no class_labels are given, we obtain the set of unique class labels from
    # the union of the ground truth annotation and the prediction
    if not class_labels:
        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))

    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=int)

    # for each correct class (row),
    # compute how many instances are predicted for each class (columns)

    for i, true_label in enumerate(class_labels):
      for j, pred_label in enumerate(class_labels):
        confusion[i, j] = np.sum((y_gold == true_label) & (y_prediction == pred_label))

    return confusion

def precision(confusion):
    class_labels = confusion.shape[0]

    col_sums = np.sum(confusion, axis=0)

    p = np.zeros(class_labels, dtype=float)

    for i in range(class_labels):
      if col_sums[i] == 0:
        p[i] = 0
      else:
        p[i] = confusion[i, i] / col_sums[i]

    # Compute the macro-averaged precision
    macro_p = np.mean(p)

    return (p, macro_p)


def recall(confusion):
    class_labels = confusion.shape[0]
    row_sums = np.sum(confusion, axis=1)
    r = np.zeros(class_labels, dtype=float)
    for i in range(class_labels):
      if row_sums[i] == 0:
        r[i] = 0
      else:
        r[i] = confusion[i, i] / row_sums[i]

    # Compute the macro-averaged recall
    macro_r = np.mean(r)
    return (r, macro_r)

def f1_score(confusion):

    (precisions, macro_p) = precision(confusion)
    (recalls, macro_r) = recall(confusion)

    # just to make sure they are of the same length
    assert len(precisions) == len(recalls)
    f = np.zeros((len(precisions), ))
    f = (2 * precisions * recalls) / (precisions + recalls)
    macro_f = np.mean(f)
    return (f, macro_f)

def accuracy_from_confusion(confusion):
    if np.sum(confusion) > 0:
        return np.trace(confusion) / np.sum(confusion)
    else:
        return 0.

def evaluate(test_db, trained_tree):
    """
    test_db: tuple (x_test, y_test)
    trained_tree: DecisionTree object already trained
    returns: accuracy (float)
    """
    x_test, y_test = test_db
    predictions = trained_tree.predict(x_test)
    acc = np.sum(predictions == y_test) / len(y_test)
    return acc

def cross_validate(dataset_x, dataset_y, k=10, seed=None):
    rng = default_rng(seed)
    n = len(dataset_y)
    indices = rng.permutation(n)
    folds = np.array_split(indices, k)

    accuracies = []
    confusion_total = None

    for i in range(k):
        test_idx = folds[i]
        train_idx = np.concatenate([folds[j] for j in range(k) if j != i])

        x_train = dataset_x[train_idx]
        y_train = dataset_y[train_idx]
        x_test = dataset_x[test_idx]
        y_test = dataset_y[test_idx]

        tree = DecisionTree()
        tree.fit(x_train, y_train)

        # Use evaluate() as per spec
        acc = evaluate((x_test, y_test), tree)
        accuracies.append(acc)

        # build confusion matrix
        preds = tree.predict(x_test)
        cm = confusion_matrix(y_test, preds)
        if confusion_total is None:
            confusion_total = cm
        else:
            confusion_total += cm

    # average metrics
    avg_acc = np.mean(accuracies)
    per_class_p, macro_p = precision(confusion_total)
    per_class_r, macro_r = recall(confusion_total)
    per_class_f1, macro_f1 = f1_score(confusion_total)

    print("Cross-validation results:")
    print(f"Accuracy: {avg_acc:.4f}")
    print("Confusion matrix:\n", confusion_total)
    print("Precision per class:", per_class_p)
    print("Recall per class:", per_class_r)
    print("F1 per class:", per_class_f1)
    print(f"Macro Precision: {macro_p:.4f}, Macro Recall: {macro_r:.4f}, Macro F1: {macro_f1:.4f}")


(clean_x, clean_y, _) = read_dataset("sample_data/clean_dataset.txt")
(noisy_x, noisy_y, _) = read_dataset("sample_data/noisy_dataset.txt")

print("\nClean dataset:")
cross_validate(clean_x, clean_y, k=10, seed=42)

print("\nNoisy dataset:")
cross_validate(noisy_x, noisy_y, k=10, seed=42)
